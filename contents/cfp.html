<!DOCTYPE HTML>

<html>
	<head>
		<title>SiMLA2026 Workshop</title>
		<meta name="description" content="Homepage of SiMLA workshop" />
		<meta name="keyword" content="Security in Machine Learning and its Applications">
		<link rel="stylesheet" type="text/css" href="../css/style.css">
	</head>
	<body>

		<div class="container">
			<header id="main_header">
				<h1>8th International Workshop on Security in Machine Learning and its Applications (SiMLA) [under development]</h1>
				<h2><a href="https://acns2026.github.io/" target="_blank">SiMLA2026 in conjunction with ACNS2026 (June 22nd-25th 2026), Stony Brook, New York, USA </a></h2>
			</header>

			<nav id="navbar">
				<ul>
					<li> <a href="../index.html">Home</a></li>
					<!-- <li> <a href="contents/program.html">Program</a></li> -->
					<li> <a href="cfp.html">Call for Papers</a></li>
					<li> <a href="author.html">Author Instructions</a></li>
					<!-- <li> <a href="contents/committee.html">Committee</a></li> -->
					<!-- <li> <a href="contents/keynote.html">Keynote</a></li> --> 
					<!-- <li> <a href="http://acns2025.fordaysec.de/registration/">Registration</a></li> -->
					<li> <a href="past_events.html">Past Events</a></li> 
				</ul>
			</nav>

			<!-- TBA CFP
			<div class="highlight">
				<h3><a href="../files/CIMSS23_CFP.pdf"> <em>Click here for a CFP flyer. Please feel free to print and distribute it! </em></a></h3>
			</div>
			-->

			<!-- <h3>News</h3>
			<p><em>The submission portal is Closed!</em></p> -->

			<!--
			<h3>Important Dates</h3>
			<ul>
				<li> <em>Workshop date: TBD during June 22-25, 2026</em></li>
				<li> Workshop Paper Submission Deadline: March 20, 2026</li>
				<li> Notification of Acceptance/Rejection: April 20, 2026</li>
				<li> Submission of camera-ready papers for pre-proceedings: May 1, 2026</li>
			</ul>
			-->

			<h3>Workshop Description</h3>
			<p align="justify">With the rapid advancement of computing hardware, learning algorithms, and the explosive growth of data, machine learning (ML) technologies have been widely adopted across diverse domains such as facial recognition, intelligent video surveillance, autonomous driving, and beyond. Despite their remarkable success, the security and privacy implications of ML systems remain insufficiently understood. In particular, adversarial machine learning continues to expose unique vulnerabilities in models, yet there is still a lack of systematic methods to assess and improve their robustness. At the same time, growing public awareness of data privacy raises critical concerns about how to train and deploy ML models without compromising sensitive information.</p>

			<p align="justify">In addition, the rapid progress toward increasingly Artificial General Intelligence (AGI) systems (e.g., foundation models and AI agents) introduces new risks that go beyond conventional ML settings. These include issues of content provenance, detecting and mitigating mis/disinformation, ensuring trustworthy deployment at scale, and safeguarding both AGI and AI agents against misuse or adversarial manipulation.</p>

			<p align="justify">This workshop aims to bring together researchers and practitioners to explore these pressing challenges. We solicit high-quality contributions addressing a wide range of topics, including but not limited to adversarial learning, robust algorithm design and evaluation, privacy-preserving machine learning techniques, and secure ML system deployment. The workshop will provide a forum for participants to exchange cutting-edge ideas, present novel solutions, and discuss emerging trends that bridge theoretical advances with real-world applications.</p>

			<h2> Topics of Interest </h2>
			Topics of interest include, but are not limited to:
			<ul>
				<li> ML for Security Applications --- software vulnerability detection, malware classification, spam/phishing detection</li>
				<li> Security Testing and Verification of ML Systems</li>
				<li> Adversarial Machine Learning --- attacks, defenses, and evaluation methodologies</li>
				<li> Robustness Analysis and Certified Defenses for ML Models</li>
				<li> Data Poisoning and Backdoor Attacks --- detection, mitigation, and resilience strategies </li>
				<li> Intrusion/Anomaly Detection/Prevention </li>
				<li> Privacy-Preserving ML --- scalable and privacy-enhancing techniques for ML such as differential privacy, secure multi-party computation, homomorphic encryption, and trusted hardware </li>
				<li> Watermarking, Ownership Verification, and Model Provenance </li>
				<li> Attacks and Defenses for Biometric Systems --- face, voice, and multimodal authentication </li>
				<li> ML for Program Analysis and Secure Software Engineering </li>
				<li> Emerging Threats in AI-Driven Systems --- autonomous systems, IoT, CPS </li>
				<li> Security of Large Language Models --- prompt injection, jailbreaks, and LLM-based security tools </li>
				<li> Security and Trustworthiness of AGI Systems --- content provenance, misinformation/disinformation detection, large-scale alignment and resilience </li>
			</ul>

			<p align="justify"> Submissions must not substantially duplicate work that any of the authors has published elsewhere or has submitted in parallel to any other venue with formally published proceedings.  Information about submissions may be shared with program chairs of other conferences for that purpose. Submissions must be anonymous, with no author names, affiliations, acknowledgement or obvious references.  Each submission must begin with a title, short abstract, and a list of keywords. The introduction should summarize the contributions of the paper at a level appropriate for a non-specialist reader.  All submissions must follow the original <a href="https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines" target="_blank">LNCS format</a> with a page limit of <b>20</b> pages (incl. references). Authors of accepted papers must guarantee that their paper will be presented at the conference and must make a full version of their paper available online.  Submissions not meeting the submission guidelines risk rejection without consideration of their merits. It is strongly encouraged that submissions be processed in LaTeX.</p>
			<p> <b>Work-in-progress papers are welcome</b>, and such submissions can expectedly be less than the maximum page limit of 20 pages. Submissions less than 10 pages, however, are discouraged as there is not enough space for appropriate technical content presentation. Authors are required to include "WiP:" in the title of a work-in-progress paper submission.</p>
			
			<p> A <em> best paper award </em> will be selected from all workshops in conjunction with ACNS2023.</p>
			<p> <b>The accepted papers will have post-proceedings published by Springer in the LNCS series. </b></p>

			<footer id="main_footer">
				<img src="../images/acns-logo_L.jpg" alt="ACNS Logo" width="150" />
				<img src="../images/LNCS-Logo.jpg" alt="LNCS Logo" width="300" />
				<img src="../images/griffith.png" alt="LNCS Logo" width="300" />
				<img src="../images/hkploy.png" alt="LNCS Logo" width="300" />
				<img src="../images/RMIT.png" alt="LNCS Logo" width="300" />
			</footer>

		</div>
	</body>
</html>
